about the ai now institute the ai now institute at new york university is an interdisciplinary research institute dedicated to understanding the social implications of ai technologies it is the first university research center focused specifically on ais social significance founded and led by kate crawford and meredith whittaker ai now is one of the few womenled ai institutes in the world ai now works with a broad coalition of stakeholders including academic researchers industry civil society policy makers and affected communities to identify and address issues raised by the rapid introduction of ai across core social domains ai now produces interdisciplinary research to help ensure that ai systems are accountable to the communities and contexts they are meant to serve and that they are applied in ways that promote justice and equity the institutes current research agenda focuses on four core areas bias and inclusion rights and liberties labor and automation and safety and critical infrastructure our most recent publications include litigating algorithms a major report assessing recent court cases focused on government use of algorithms anatomy of an ai system a largescale map and longform essay produced in partnership with share lab which investigates the human labor data and planetary resources required to operate an amazon echo algorithmic impact assessment aia report which helps affected communities and stakeholders assess the use of ai and algorithmic decisionmaking in public agencies algorithmic accountability policy toolkit which is geared toward advocates interested in understanding government use of algorithmic systems we also host expert workshops and public events on a wide range of topics our workshop on immigration data and automation in the trump era cohosted with the brennan center for justice and the center for privacy and technology at georgetown law focused on the trump administrations use of data harvesting predictive analytics and machine learning to target immigrant communities the data genesis working group convenes experts from across industry and academia to examine the mechanics of dataset provenance and maintenance our roundtable on machine learning inequality and bias cohosted in berlin with the robert bosch academy gathered researchers and policymakers from across europe to address issues of bias discrimination and fairness in machine learning and related technologies our annual public symposium convenes leaders from academia industry government and civil society to examine the biggest challenges we face as ai moves into our everyday lives the ai now symposium addressed the intersection of ai ethics organizing and accountability examining the landmark events of the past year over people registered for the event which was free and open to the public recordings of the program are available on our website more information is available at wwwainowinstituteorg recommendations governments need to regulate ai by expanding the powers of sectorspecific agencies to oversee audit and monitor these technologies by domain the implementation of ai systems is expanding rapidly without adequate governance oversight or accountability regimes domains like health education criminal justice and welfare all have their own histories regulatory frameworks and hazards however a national ai safety body or general ai standards and certification model will struggle to meet the sectoral expertise requirements needed for nuanced regulation we need a sectorspecific approach that does not prioritize the technology but focuses on its application within a given domain useful examples of sectorspecific approaches include the united states federal aviation administration and the national highway traffic safety administration facial recognition and affect recognition need stringent regulation to protect the public interest such regulation should include national laws that require strong oversight clear limitations and public transparency communities should have the right to reject the application of these technologies in both public and private contexts mere public notice of their use is not sufficient and there should be a high threshold for any consent given the dangers of oppressive and continual mass surveillance affect recognition deserves particular attention affect recognition is a subclass of facial recognition that claims to detect things such as personality inner feelings mental health and worker engagement based on images or video of faces these claims are not backed by robust scientific evidence and are being applied in unethical and irresponsible ways that often recall the pseudosciences of phrenology and physiognomy linking affect recognition to hiring access to insurance education and policing creates deeply concerning risks at both an individual and societal level the ai industry urgently needs new approaches to governance as this report demonstrates internal governance structures at most technology companies are failing to ensure accountability for ai systems government regulation is an important component but leading companies in the ai industry also need internal accountability structures that go beyond ethics guidelines this should include rankandfile employee representation on the board of directors external ethics advisory boards and the implementation of independent monitoring and transparency efforts third party experts should be able to audit and publish about key systems and companies need to ensure that their ai infrastructures can be understood from nose to tail including their ultimate application and use ai companies should waive trade secrecy and other legal claims that stand in the way of accountability in the public sector vendors and developers who create ai and automated decision systems for use in government should agree to waive any trade secrecy or other legal claim that inhibits full auditing and understanding of their software corporate secrecy laws are a barrier to due process they contribute to the black box effect rendering systems opaque and unaccountable making it hard to assess bias contest decisions or remedy errors anyone procuring these technologies for use in the public sector should demand that vendors waive these claims before entering into any agreements technology companies should provide protections for conscientious objectors employee organizing and ethical whistleblowers organizing and resistance by technology workers has emerged as a force for accountability and ethical decision making technology companies need to protect workers ability to organize whistleblow and make ethical choices about what projects they work on this should include clear policies accommodating and protecting conscientious objectors ensuring workers the right to know what they are working on and the ability to abstain from such work without retaliation or retribution workers raising ethical concerns must also be protected as should whistleblowing in the public interest consumer protection agencies should apply truthinadvertising laws to ai products and services the hype around ai is only growing leading to widening gaps between marketing promises and actual product performance with these gaps come increasing risks to both individuals and commercial customers often with grave consequences much like other products and services that have the potential to seriously impact or exploit populations ai vendors should be held to high standards for what they can promise especially when the scientific evidence to back these promises is inadequate and the longerterm consequences are unknown technology companies must go beyond the pipeline model and commit to addressing the practices of exclusion and discrimination in their workplaces technology companies and the ai field as a whole have focused on the pipeline model looking to train and hire more diverse employees while this is important it overlooks what happens once people are hired into workplaces that exclude harass or systemically undervalue people on the basis of gender race sexuality or disability companies need to examine the deeper issues in their workplaces and the relationship between exclusionary cultures and the products they build which can produce tools that perpetuate bias and discrimination this change in focus needs to be accompanied by practical action including a commitment to end pay and opportunity inequity along with transparency measures about hiring and retention fairness accountability and transparency in ai require a detailed account of the full stack supply chain for meaningful accountability we need to better understand and track the component parts of an ai system and the full supply chain on which it relies that means accounting for the origins and use of training data test data models application program interfaces apis and other infrastructural components over a product life cycle we call this accounting for the full stack supply chain of ai systems and it is a necessary condition for a more responsible form of auditing the full stack supply chain also includes understanding the true environmental and labor costs of ai systems this incorporates energy use the use of labor in the developing world for content moderation and training data creation and the reliance on clickworkers to develop and maintain ai systems more funding and support are needed for litigation labor organizing and community participation on ai accountability issues the people most at risk of harm from ai systems are often those least able to contest the outcomes we need increased support for robust mechanisms of legal redress and civic participation this includes supporting public advocates who represent those cut off from social services due to algorithmic decision making civil society organizations and labor organizers that support groups that are at risk of job loss and exploitation and communitybased infrastructures that enable public participation university ai programs should expand beyond computer science and engineering disciplines ai began as an interdisciplinary field but over the decades has narrowed to become a technical discipline with the increasing application of ai systems to social domains it needs to expand its disciplinary orientation that means centering forms of expertise from the social and humanistic disciplines ai efforts that genuinely wish to address social implications cannot stay solely within computer science and engineering departments where faculty and students are not trained to research the social world expanding the disciplinary orientation of ai research will ensure deeper attention to social contexts and more focus on potential hazards when these systems are applied to human populations executive summary at the core of the cascading scandals around ai in are questions of accountability who is responsible when ai systems harm us how do we understand these harms and how do we remedy them where are the points of intervention and what additional research and regulation is needed to ensure those interventions are effective currently there are few answers to these questions and the frameworks presently governing ai are not capable of ensuring accountability as the pervasiveness complexity and scale of these systems grow the lack of meaningful accountability and oversight including basic safeguards of responsibility liability and due process is an increasingly urgent concern building on our and reports the ai now report contends with this central problem and addresses the following key issues the growing accountability gap in ai which favors those who create and deploy these technologies at the expense of those most affected the use of ai to maximize and amplify surveillance especially in conjunction with facial and affect recognition increasing the potential for centralized control and oppression increasing government use of automated decision systems that directly impact individuals and communities without established accountability structures unregulated and unmonitored forms of ai experimentation on human populations the limits of technological solutions to problems of fairness bias and discrimination within each topic we identify emerging challenges and new research and provide recommendations regarding ai development deployment and regulation we offer practical pathways informed by research so that policymakers the public and technologists can better understand and mitigate risks given that the ai now institutes location and regional expertise is concentrated in the us this report will focus primarily on the us context which is also where several of the worlds largest ai companies are based the ai accountability gap is growing the technology scandals of have shown that the gap between those who develop and profit from aiand those most likely to suffer the consequences of its negative effectsis growing larger not smaller there are several reasons for this including a lack of government regulation a highly concentrated ai sector insufficient governance structures within technology companies power asymmetries between companies and the people they serve and a stark cultural divide between the engineering cohort responsible for technical research and the vastly diverse populations where ai systems are deployed these gaps are producing growing concern about bias discrimination due process liability and overall responsibility for harm this report emphasizes the urgent need for stronger sectorspecific research and regulation ai is amplifying widespread surveillance the role of ai in widespread surveillance has expanded immensely in the us china and many other countries worldwide this is seen in the growing use of sensor networks social media tracking facial recognition and affect recognition these expansions not only threaten individual privacy but accelerate the automation of surveillance and thus its reach and pervasiveness this presents new dangers and magnifies many longstanding concerns the use of affect recognition based on debunked pseudoscience is also on the rise affect recognition attempts to read inner emotions by a close analysis of the face and is connected to spurious claims about peoples mood mental health level of engagement and guilt or innocence this technology is already being used for discriminatory and unethical purposes often without peoples knowledge facial recognition technology poses its own dangers reinforcing skewed and potentially discriminatory practices from criminal justice to education to employment and presents risks to human rights and civil liberties in multiple countries governments are rapidly expanding the use of automated decision systems without adequate protections for civil rights around the world government agencies are procuring and deploying automated decision systems ads under the banners of efficiency and costsavings yet many of these systems are untested and poorly designed for their tasks resulting in illegal and often unconstitutional violations of individual rights worse when they make errors and bad decisions the ability to question contest and remedy these is often difficult or impossible some agencies are attempting to provide mechanisms for transparency due process and other basic rights but trade secrecy and similar laws threaten to prevent auditing and adequate testing of these systems drawing from proactive agency efforts and from recent strategic litigation we outline pathways for ads accountability rampant testing of ai systems in the wild on human populations silicon valley is known for its move fast and break things mentality whereby companies are pushed to experiment with new technologies quickly and without much regard for the impact of failures including who bears the risk in the past year we have seen a growing number of experiments deploying ai systems in the wild without proper protocols for notice consent or accountability such experiments continue due in part to a lack of consequences for failure when harms occur it is often unclear where or with whom the responsibility lies researching and assigning appropriate responsibility and liability remains an urgent priority the limits of technological fixes to problems of fairness bias and discrimination much new work has been done designing mathematical models for what should be considered fair when machines calculate outcomes aimed at avoiding discrimination yet without a framework that accounts for social and political contexts and histories these mathematical formulas for fairness will almost inevitably miss key factors and can serve to paper over deeper problems in ways that ultimately increase harm or ignore justice broadening perspectives and expanding research into ai fairness and bias beyond the merely mathematical is critical to ensuring we are capable of addressing the core issues and moving the focus from parity to justice the move to ethical principles this year saw the emergence of numerous ethical principles and guidelines for the creation and deployment of ai technologies many in response to growing concerns about ais social implications but as studies show these types of ethical commitments have little measurable effect on software development practices if they are not directly tied to structures of accountability and workplace practices further these codes and guidelines are rarely backed by enforcement oversight or consequences for deviation ethical codes can only help close the ai accountability gap if they are truly built into the processes of ai development and are backed by enforceable mechanisms of responsibility that are accountable to the public interest the following report develops these themes in detail reflecting on the latest academic research and outlines seven strategies for moving forward expanding ai fairness research beyond a focus on mathematical parity and statistical fairness toward issues of justice studying and tracking the full stack of infrastructure needed to create ai including accounting for material supply chains accounting for the many forms of labor required to create and maintain ai systems committing to deeper interdisciplinarity in ai analyzing race gender and power in ai developing new policy interventions and strategic litigation building coalitions between researchers civil society and organizers within the technology sector these approaches are designed to positively recast the ai field and address the growing power imbalance that currently favors those who develop and profit from ai systems at the expense of the populations most likely to be harmed introduction the social challenges of ai in the past year has seen accelerated integration of powerful artificial intelligence systems into core social institutions against a backdrop of rising inequality political populism and industry scandals there have been major movements from both inside and outside technology companies pushing for greater accountability and justice the ai now report focuses on these themes and examines the gaps between ai ethics and meaningful accountability and the role of organizing and regulation in short it has been a dramatic year in ai in any normal year cambridge analytica seeking to manipulate national elections in the us and uk using social media data and algorithmic ad targeting would have been the biggest story but in it was just one of many scandals facebook had a series of disasters including a massive data breach in september multiple class action lawsuits for discrimination accusations of inciting ethnic cleansing in myanmar potential violations of the fair housing act and hosting masses of fake russian accounts throughout the year the companys executives were frequently summoned to testify with mark zuckerberg facing the us senate in april and the european parliament in may zuckerberg mentioned ai technologies over times in his congressional testimony as the cureall to the companys problems particularly in the complex areas of censorship fairness and content moderation but facebook wasnt the only one in crisis news broke in march that google was building ai systems for the department of defenses drone surveillance program project maven the news kicked off an unprecedented wave of technology worker organizing and dissent across the industry in june when the trump administration introduced the family separation policy that forcibly removed immigrant children from their parents employees from amazon salesforce and microsoft all asked their companies to end contracts with us immigration and customs enforcement ice less than a month later it was revealed that ice modified its own risk assessment algorithm so that it could only produce one result the system recommended detain for of immigrants in custody throughout the year ai systems continued to be tested on live populations in highstakes domains with some serious consequences in march autonomous cars killed drivers and pedestrians then in may a voice recognition system in the uk designed to detect immigration fraud ended up cancelling thousands of visas and deporting people in error documents leaked in july showed that ibm watson was producing unsafe and incorrect cancer treatment recommendations and an investigation in september revealed that ibm was also working with the new york city police department nypd to build an ethnicity detection feature to search faces based on race using police camera footage of thousands of people in the streets of new york taken without their knowledge or permission this is just a sampling of an extraordinary series of incidents from the response has included a growing wave of criticism with demands for greater accountability from the technology industry and the systems they build in turn some companies have made public calls for the us to regulate technologies like facial recognition others have published ai ethics principles and increased efforts to produce technical fixes for issues of bias and discrimination in ai systems but many of these ethical and technical approaches define the problem space very narrowly neither contending with the historical or social context nor providing mechanisms for public accountability oversight and due process this makes it nearly impossible for the public to validate that any of the current problems have in fact been addressed as numerous scholars have noted one significant barrier to accountability is the culture of industrial and legal secrecy that dominates ai development just as many ai technologies are black boxes so are the industrial cultures that create them many of the fundamental building blocks required to understand ai systems and to ensure certain forms of accountability from training data to data models to the code dictating algorithmic functions to implementation guidelines and software to the business decisions that directed design and development are rarely accessible to review hidden by corporate secrecy laws the current accountability gap is also caused by the incentives driving the rapid pace of technical ai research the push to innovate publish first and present a novel addition to the technical domain has created an accelerated cadence in the field of ai and in technical disciplines more broadly this comes at the cost of considering empirical questions of context and use or substantively engaging with ethical concerns similarly technology companies are driven by pressures to launch and iterate which assume complex social and political questions will be handled by policy and legal departments leaving developers and sales departments free from the responsibility of considering the potential downsides the move fast and break things culture provides little incentive for ensuring meaningful public accountability or engaging the communities most likely to experience harm this is particularly problematic as the accelerated application of ai systems in sensitive social and political domains presents risks to marginalized communities the challenge to create better governance and greater accountability for ai poses particular problems when such systems are woven into the fabric of government and public institutions the lack of transparency notice meaningful engagement accountability and oversight creates serious structural barriers for due process and redress for unjust and discriminatory decisions in this years report we assess many pressing issues facing us as ai tools are deployed further into the institutions that govern everyday life we focus on the biggest industry players because the number of companies able to create ai at scale is very small while their power and reach is global we evaluate the current range of responses from industry governments researchers activists and civil society at large we suggest a series of substantive approaches and make ten specific recommendations finally we share the latest research and policy strategies that can contribute to greater accountability as well as a richer understanding of ai systems in a wider social context the intensifying problem space in identifying the most pressing social implications of ai this year we look closely at the role of ai in widespread surveillance in multiple countries around the world and at the implications for rights and liberties in particular we consider the increasing use of facial recognition and a subclass of facial recognition known as affect recognition and assess the growing calls for regulation next we share our findings on the government use of automated decision systems and what questions this raises for fairness transparency and due process when such systems are protected by trade secrecy and other laws that prevent auditing and close examination finally we look at the practices of deploying experimental systems in the wild testing them on human populations we analyze who has the most to gain and who is at greatest risk of experiencing harm ai is amplifying widespread surveillance this year we have seen ai amplify largescale surveillance through techniques that analyze video audio images and social media content across entire populations and identify and target individuals and groups while researchers and advocates have long warned about the dangers of mass data collection and surveillance ai raises the stakes in three areas automation scale of analysis and predictive capacity specifically ai systems allow automation of surveillance capabilities far beyond the limits of human review and handcoded analytics thus they can serve to further centralize these capabilities in the hands of a small number of actors these systems also exponentially scale analysis and tracking across large quantities of data attempting to make connections and inferences that would have been difficult or impossible before their introduction finally they provide new predictive capabilities to make determinations about individual character and risk profiles raising the possibility of granular population controls china has offered several examples of alarming aienabled surveillance this year which we know about largely because the government openly acknowledges them however its important to note that many of the same infrastructures already exist in the us and elsewhere often produced and promoted by private companies whose marketing emphasizes beneficial use cases in the us the use of these tools by law enforcement and government is rarely open to public scrutiny as we will review and there is much we do not know such infrastructures and capabilities could easily be turned to more surveillant ends in the us without public disclosure and oversight depending on market incentives and political will in china military and statesanctioned automated surveillance technology is being deployed to monitor large portions of the population often targeting marginalized groups reports include installation of facial recognition tools at the hong kongshenzhen border using flocks of robotic dovelike drones in five provinces across the country and the widely reported social credit monitoring system each of which illustrates how aienhanced surveillance systems can be mobilized as a means of farreaching social control the most oppressive use of these systems is reportedly occuring in the xinjiang autonomous region described by the economist as a police state like no other surveillance in this uighur ethnic minority area is pervasive ranging from physical checkpoints and programs where uighur households are required to adopt han chinese officials into their family to the widespread use of surveillance cameras spyware wifi sniffers and biometric data collection sometimes by stealth machine learning tools integrate these streams of data to generate extensive lists of suspects for detention in reeducation camps built by the government to discipline the group estimates of the number of people detained in these camps range from hundreds of thousands to nearly one million these infrastructures are not unique to china venezuela announced the adoption of a new smart card id known as the carnet de patria which by integrating government databases linked to social programs could enable the government to monitor citizens personal finances medical history and voting activity in the united states we have seen similar efforts the pentagon has funded research on aienabled social media surveillance to help predict largescale population behaviors and the us immigration and customs enforcement ice agency is using an investigative case management system developed by palantir and powered by amazon web services in its deportation operations the system integrates public data with information purchased from private data brokers to create profiles of immigrants in order to aid the agency in profiling tracking and deporting individuals these examples show how ai systems increase integration of surveillance technologies into datadriven models of social control and amplify the power of such data magnifying the stakes of misuse and raising urgent and important questions as to how basic rights and liberties will be protected the faulty science and dangerous history of affect recognition we are also seeing new risks emerging from unregulated facial recognition systems these systems facilitate the detection and recognition of individual faces in images or video and can be used in combination with other tools to conduct more sophisticated forms of surveillance such as automated lipreading offering the ability to observe and interpret speech from a distance among a host of aienabled surveillance and tracking techniques facial recognition raises particular civil liberties concerns because facial features are a very personal form of biometric identification that is extremely difficult to change it is hard to subvert or opt out of its operations and unlike other tracking tools facial recognition seeks to use ai for much more than simply recognizing faces once identified a face can be linked with other forms of personal records and identifiable data such as credit score social graph or criminal record affect recognition a subset of facial recognition aims to interpret faces to automatically detect inner emotional states or even hidden intentions this approach promises a type of emotional weather forecasting analyzing hundreds of thousands of images of faces detecting microexpressions and mapping these expressions to true feelings this reactivates a long tradition of physiognomy a pseudoscience that claims facial features can reveal innate aspects of our character or personality dating from ancient times scientific interest in physiognomy grew enormously in the nineteenth century when it became a central method for scientific forms of racism and discrimination although physiognomy fell out of favor following its association with nazi race science researchers are worried about a reemergence of physiognomic ideas in affect recognition applications the idea that ai systems might be able to tell us what a student a customer or a criminal suspect is really feeling or what type of person they intrinsically are is proving attractive to both corporations and governments even though the scientific justifications for such claims are highly questionable and the history of their discriminatory purposes welldocumented the case of affect detection reveals how machine learning systems can easily be used to intensify forms of classification and discrimination even when the basic foundations of these theories remain controversial among psychologists the scientist most closely associated with aienabled affect detection is the psychologist paul ekman who asserted that emotions can be grouped into a small set of basic categories like anger disgust fear happiness sadness and surprise studying faces according to ekman produces an objective reading of authentic interior statesa direct window to the soul underlying his belief was the idea that emotions are f ixed and universal identical across individuals and clearly visible in observable biological mechanisms regardless of cultural context but ekmans work has been deeply criticized by psychologists anthropologists and other researchers who have found his theories do not hold up under sustained scrutiny the psychologist lisa feldman barrett and her colleagues have argued that an understanding of emotions in terms of these rigid categories and simplistic physiological causes is no longer tenable nonetheless ai researchers have taken his work as fact and used it as a basis for automating emotion detection contextual social and cultural factors how where and by whom such emotional signifiers are expressed play a larger role in emotional expression than was believed by ekman and his peers in light of this new scientific understanding of emotion any simplistic mapping of a facial expression onto basic emotional categories through ai is likely to reproduce the errors of an outdated scientific paradigm it also raises troubling ethical questions about locating the arbiter of someones real character and emotions outside of the individual and the potential abuse of power that can be justified based on these faulty claims psychiatrist jamie metzl documents a recent cautionary example a pattern in the s of diagnosing black people with schizophrenia if they supported the civil rights movement affect detection combined with largescale facial recognition has the potential to magnify such political abuses of psychological profiling in the realm of education some us universities have considered using affect analysis software on students the university of st thomas in minnesota looked at using a system based on microsofts facial recognition and affect detection tools to observe students in the classroom using a webcam the system predicts the students emotional state an overview of student sentiment is viewable by the teacher who can then shift their teaching in a way that ensures student engagement as judged by the system this raises serious questions on multiple levels what if the system with a simplistic emotional model simply cannot grasp more complex states how would a student contest a determination made by the system what if different students are seen as happy while others are angryhow should the teacher redirect the lesson what are the privacy implications of such a system particularly given that in the case of the pilot program there is no evidence that students were informed of its use on them outside of the classroom we are also seeing personal assistants like alexa and siri seeking to pick up on the emotional undertones of human speech with companies even going so far as to patent methods of marketing based on detecting emotions as well as mental and physical health the aienabled emotion measurement company affectiva now promises it can promote safer driving by monitoring driver and occupant emotions cognitive states and reactions to the driving experiencefrom face and voice yet there is little evidence that any of these systems actually work across different individuals contexts and cultures or have any safeguards put in place to mitigate concerns about privacy bias or discrimination in their operation furthermore as we have seen in the large literature on bias and fairness classifications of this nature not only have direct impacts on human lives but also serve as data to train and influence other ai systems this raises the stakes for any use of affect recognition further emphasizing why it should be critically examined and its use severely restricted facial recognition amplifies civil rights concerns concerns are intensifying that facial recognition increases racial discrimination and other biases in the criminal justice system earlier this year the american civil liberties union aclu disclosed that both the orlando police department and the washington county sheriffs department were using amazons rekognition system which boasts that it can perform realtime face recognition across tens of millions of faces and detect up to faces in challenging crowded photos in washington county amazon specifically worked with the sheriffs department to create a mobile app that could scan faces and compare them against a database of at least mugshots an amazon representative recently revealed during a talk that they have been considering applications where orlandos network of surveillance cameras could be used in conjunction with facial recognition technology to find a person of interest wherever they might be in the city in addition to the privacy and mass surveillance concerns commonly raised the use of facial recognition in law enforcement has also intersected with concerns of racial and other biases researchers at the aclu and the university of california uc berkeley tested amazons rekognition tool by comparing the photos of sitting members in the united states congress with a database containing photos of people who had been arrested the results showed significant levels of inaccuracy amazons rekognition incorrectly identified members of congress as people from the arrest database moreover the false positives disproportionately occurred among nonwhite members of congress with an error rate of nearly compared to only for white members such results echo a string of findings that have demonstrated that facial recognition technology is on average better at detecting lightskinned people than darkskinned people and better at detecting men than women in its response to the aclu amazon acknowledged that the rekognition results can be significantly skewed by using a facial database that is not appropriately representative given the deep and historical racial biases in the criminal justice system most law enforcement databases are unlikely to be appropriately representative despite these serious flaws ongoing pressure from civil rights groups and protests from amazon employees over the potential for misuse of these technologies amazon web services ceo andrew jassy recently told employees that we feel really great and really strongly about the value that amazon rekognition is providing our customers of all sizes and all types of industries in law enforcement and out of law enforcement nor is amazon alone in implementing facial recognition technologies in unaccountable ways investigative journalists recently disclosed that ibm and the new york city police department nypd partnered to develop such a system that included ethnicity search as a custom feature trained on thousands of hours of nypd surveillance footage use of facial recognition software in the private sector has expanded as well major retailers and venues have already begun using these technologies to detect shoplifters monitor crowds and even scan for unhappy customers using facial recognition systems instrumented with affect detection capabilities these concerns are amplified by a lack of laws and regulations there is currently no federal legislation that seeks to provide standards restrictions requirements or guidance regarding the development or use of facial recognition technology in fact most existing federal legislation looks to promote the use of facial recognition for surveillance immigration enforcement employment verification and domestic entryexit systems the laws that we do have are piecemeal and none specifically address facial recognition among these is the biometric information privacy act a illinois law that sets forth stringent rules regarding the collection of biometrics while the law does not mention facial recognition given that the technology was not widely available in many of its requirements such as obtaining consent are reasonably interpreted to apply more recently several municipalities and a local transit system have adopted ordinances that seek to create greater transparency and oversight of data collection and use requirements regarding the acquisition of surveillance technologies which would include facial recognition based on the expansive definition in these ordinances opposition to the use of facial recognition tools by government agencies is growing earlier this year ai now joined the aclu and over other research and advocacy organizations calling on amazon to stop selling facial recognition software to government agencies after the aclu uncovered documents showing law enforcement use of amazons rekognition api members of congress are also pushing amazon to provide more information some have gone further calling for an outright ban scholars woodrow hartzog and evan selinger argue that facial recognition technology is a tool for oppression thats perfectly suited for governments to display unprecedented authoritarian control and an allout privacyeviscerating machine necessitating extreme caution and diligence before being applied in our contemporary digital ecosystem critiquing the stanford gaydar study that claimed its deep neural network was more accurate than humans at predicting sexuality from facial images frank pasquale wrote that there are some scientific research programs best not pursued and this might be one of them kade crockford director of the technology for liberty program at aclu of massachusetts also wrote in favor of a ban stating that artificial intelligence technologies like face recognition systems fundamentally change the balance of power between the people and the governmentsome technologies are so dangerous to that balance of power that they must be rejected microsoft president brad smith has called for government regulation of facial recognition while rick smith ceo of law enforcement technology company axon recently stated that the accuracy thresholds of facial recognition tools arent where they need to be to be making operational decisions the events of this year have strongly underscored the urgent need for stricter regulation of both facial and affect recognition technologies such regulations should severely restrict use by both the public and the private sector and ensure that communities affected by these technologies are the final arbiters of whether they are used at all this is especially important in situations where basic rights and liberties are at risk requiring stringent oversight audits and transparency linkages should not be permitted between private and government databases at this point given the evidence in hand policymakers should not be funding or furthering the deployment of these systems in public spaces the risks of automated decision systems in government over the past year we have seen a substantial increase in the adoption of automated decision systems ads across government domains including criminal justice child welfare education and immigration often adopted under the theory that they will improve government efficiency or costsavings ads seek to aid or replace various decisionmaking processes and policy determinations however because the underlying models are often proprietary and the systems frequently untested before deployment many community advocates have raised significant concerns about lack of due process accountability community engagement and auditing such was the case for tammy dobbs who moved to arkansas in and signed up for a state disability program to help her with her cerebral palsy under the program the state sent a qualified nurse to assess tammy to determine the number of caregiver hours she would need because tammy spent most of her waking hours in a wheelchair and had stiffness in her hands her initial assessment allocated hours of home care per week fast forward to when the state assessor arrived with a new ads on her laptop using a proprietary algorithm this system calculated the number of hours tammy would be allotted without any explanation or opportunity for comment discussion or reassessment the program allotted tammy hours per week a massive and sudden drop that tammy had no chance to prepare for and that severely reduced her quality of life nor was tammys situation exceptional according to legal aid of arkansas attorney kevin de liban hundreds of other individuals with disabilities also received dramatic reductions in hours all without any meaningful opportunity to understand or contest their allocations legal aid subsequently sued the state of arkansas eventually winning a ruling that the new algorithmic allocation program was erroneous and unconstitutional yet by then much of the damage to the lives of those affected had been done the arkansas disability cases provide a concrete example of the substantial risks that occur when governments use ads in decisions that have immediate impacts on vulnerable populations while individual assessors may also suffer from bias or flawed logic the impact of their casebycase decisions has nowhere near the magnitude or scale that a single flawed ads can have across an entire population the increased introduction of such systems comes at a time when according to the world income inequality database the united states has the highest income inequality rate of all western countries moreover federal reserve data shows wealth inequalities continue to grow and racial wealth disparities have more than tripled in the last years with current policies set to exacerbate such problems in alone we have seen a us executive order cutting funding for social programs that serve the countrys poorest citizens alongside a proposed federal budget that will significantly reduce lowincome and affordable housing the implementation of onerous work requirements for medicaid and a proposal to cut food assistance benefits for lowincome seniors and people with disabilities in the context of such policies agencies are under immense pressure to cut costs and many are looking to ads as a means of automating hard decisions that have very real effects on those most in need as such many ads systems are often implemented with the goal of doing more with less in the context of austerity policies and costcutting they are frequently designed and configured primarily to achieve these goals with their ultimate effectiveness being evaluated based on their ability to trim costs often at the expense of the populations such tools are ostensibly intended to serve as researcher virginia eubanks argues what seems like an effort to lower program barriers and remove human bias often has the opposite effect blocking hundreds of thousands of people from receiving the services they deserve when these problems arise they are frequently difficult to remedy few ads are designed or implemented in ways that easily allow affected individuals to contest mitigate or fix adverse or incorrect decisions additionally human discretion and the ability to intervene or override a systems determination is often substantially limited or removed from case managers social workers and others trained to understand the context and nuance of a particular person and situation these frontline workers become mere intermediaries communicating inflexible decisions made by automated systems without the ability to alter them unlike the civil servants who have historically been responsible for such decisions many ads come from private vendors and are frequently implemented without thorough testing review or auditing to ensure their fitness for a given domain nor are these systems typically built with any explicit form of oversight or accountability this makes discovery of problematic automated outcomes difficult especially since such errors and evidence of discrimination frequently manifest as collective harms only recognizable as a pattern across many individual cases detecting such problems requires oversight and monitoring it also requires access to data that is often neither available to advocates and the public nor monitored by government agencies for example the houston federation of teachers sued the houston independent school district for procuring a thirdparty ads to use student test data to make teacher employment decisions including which teachers were promoted and which were terminated it was revealed that no one in the district not a single employee could explain or even replicate the determinations made by the system even though the district had access to all the underlying data teachers who sought to contest the determinations were told that the black box system was simply to be believed and could not be questioned even when the teachers brought a lawsuit claiming constitutional civil rights and labor law violations the ads vendor fought against providing any access to how its system worked as a result the judge ruled that the use of this ads in public employee cases could run afoul of constitutional due process protections especially when trade secrecy blocked employees ability to understand how decisions were made the case has subsequently been settled with the district agreeing to abandon the thirdparty ads similarly in los angeles county adopted an ads to assess imminent danger or harm to children and to predict the likelihood of a family being rereferred to the child welfare system within to months the county did not perform a review of the system or assess the efficacy of using predictive analytics for child safety and welfare it was only after the death of a child whom the system failed to identify as atrisk that county leadership directed a review which raised serious questions regarding the systems validity the review specifically noted that the system failed to provide a comprehensive picture of a given family but instead focused on a few broad strokes without giving weight to important nuance virginia eubanks found similar problems in her investigation of an ads developed by the same private vendor for use in allegheny county pa this system produced biased outcomes because it significantly oversampled poor children from working class communities especially communities of color in effect subjecting poor parents and children to more frequent investigation even in the face of acknowledged issues of bias and the potential for error in highstakes domains these systems are being rapidly adopted the ministry of social development in new zealand supported the use of a predictive ads system to identify children at risk of maltreatment despite their recognizing that the system raised significant ethical concerns they defended this on the grounds that the benefits plausibly outweighed the potential harms which included reconfiguring child welfare as a statistical issue these cases not only highlight the need for greater transparency oversight and accountability in the adoption development and implementation of ads but also the need for examination of the limitations of these systems overall and of the economic and policy factors that accompany the push to apply such systems virginia eubanks who investigated allegheny countys use of an ads in child welfare looked at this and a number of case studies to show how ads are often adopted to avoid or obfuscate broader structural and systemic problems in society problems that are often beyond the capacity of cashstrapped agencies to address meaningfully other automated systems have also been proposed as a strategy to combat preexisting problems within government systems for years criminal justice advocates and researchers have pushed for the elimination of cash bail which has been shown to disproportionately harm individuals based on race and socioeconomic status while at the same time failing to enhance public safety in response new jersey and california recently passed legislation aimed at addressing this concern however instead of simply ending cash bail they replaced it with a pretrial assessment system designed to algorithmically generate risk scores that claim to predict whether a person should go free or be detained in jail while awaiting trial the shift from policies such as cash bail to automated systems and risk assessment scoring is still relatively new and is proceeding even without substantial research examining the potential to amplify discrimination within the criminal justice system yet there are some early indicators that raise concern new jerseys law went into effect in and while the state has experienced a decline in its pretrial population advocates have expressed worry that racial disparities in the risk assessment system persist similarly when californias legislation passed earlier this year many of the criminal justice advocates who pushed for the end of cash bail and supported an earlier version of the bill opposed its final version due to the risk assessment requirement education policy is also feeling the impact of automated decision systems a university college london professor is among those who argued for ai to replace standardized testing suggesting that ucl knowledge labs aiassess can be trustedwith the assessment of our childrens knowledge and understanding and can serve to replace or augment more traditional testing however much like other forms of ai there is a growing body of research that shows automated essay scoring systems may encode bias against certain linguistic and ethnic groups in ways that replicate patterns of marginalization unfair decisions based on automated scores assigned to students from historically and systemically disadvantaged groups are likely to have profound consequences on childrens lives and to exacerbate existing disparities in access to employment opportunities and resources the implications of educational ads go beyond testing to other areas such as school assignments and even transportation the city of boston was in the spotlight this year after two failed efforts to address school equity via automated systems first the school district adopted a geographicallydriven school assignment algorithm intended to provide students access to higher quality schools closer to home the citys goal was to increase the racial and geographic integration in the school district but a report assessing the impact of the system determined that it did the opposite while it shortened student commutes it ultimately reduced school integration researchers noted that this was in part because it was impossible for the system to meet its intended goal given the history and context within which it was being used the geographic distribution of quality schools in boston was already inequitable and the preexisting racial disparities that played a role in placement at these schools created complications that could not be overcome by an algorithm following this the boston school district tried again to use an algorithmic system to improve inequity this time designing it to reconfigure school start times aiming to begin high school later and middle school earlier this was done in an effort to improve student health and performance based on a recognition of students circadian rhythms at different ages and to optimize use of school buses to produce cost savings it also aimed to increase racial equity since students of color primarily attended schools with inconvenient start times compounded by long bus rides the city developed an ads that optimized for these goals however it was never implemented because of significant public backlash which ultimately resulted in the resignation of the superintendent in this case the design process failed to adequately recognize the needs of families or include them in defining and reviewing system goals under the proposed system parents with children in both high school and middle school would need to reconfigure their schedules for vastly different start and end times putting strain on those without this flexibility the national association for the advancement of colored people naacp and the lawyers committee for civil rights and economic justice opposed the plan because of the school districts failure to appreciate that parents of color and lowerincome parents often rely on jobs that lack work schedule flexibility and may not be able to afford additional child care these failed efforts demonstrate two important issues that policymakers must consider when evaluating the use of these systems first unaddressed structural and systemic problems will persist and will likely undermine the potential benefits of these systems if they are not addressed prior to a systems design and implementation second robust and meaningful community engagement is essential before a system is put in place and should be included in the process of establishing a systems goals and purpose in ai nows algorithmic impact assessment aia framework community engagement is an integral part of any ads accountability process both as part of the design stage as well as before during and after implementation when affected communities have the opportunity to assess and potentially reject the use of systems that are not acceptable and to call out fundamental f laws in the system before it is put in place the validity and legitimacy of the system is vastly improved such engagement serves communities and government agencies if parents of color and lowerincome parents in boston were meaningfully engaged in assessing the goals of the school start time algorithmic intervention their concerns might have been accounted for in the design of the system saving the city time and resources and providing a muchneeded model of oversight above all accountability in the government use of algorithmic systems is impossible when the systems making recommendations are black boxes when thirdparty vendors insist on trade secrecy to keep their systems opaque it makes any path to redress or appeal extremely difficult this is why vendors should waive trade secrecy and other legal claims that would inhibit the ability to understand audit or test their systems for bias error or other issues it is important for both people in government and those who study the effects of these systems to understand why automated recommendations are made and to be able to trust their validity it is even more critical that those whose lives are negatively impacted by these systems be able to contest and appeal adverse decisions governments should be cautious while automated decision systems may promise shortterm cost savings and efficiencies it is governments not third party vendors who will ultimately be held responsible for their failings without adequate transparency accountability and oversight these systems risk introducing and reinforcing unfair and arbitrary practices in critical government determinations and policies experimenting on society who bears the burden over the last ten years the funding and focus on technical ai research and development has accelerated but efforts at ensuring that these systems are safe and nondiscriminatory have not received the same resources or attention currently there are few established methods for measuring validating and monitoring the effects of ai systems in the wild ai systems tasked with significant decision making are effectively tested on live populations often with little oversight or a clear regulatory framework for example in march a selfdriving uber was navigating the phoenix suburbs and failed to see a woman hitting and killing her last march tesla confirmed that a second driver had been killed in an accident in which the cars autopilot technology was engaged neither company suffered serious consequences and in the case of uber the person minding the autonomous vehicle was ultimately blamed even though uber had explicitly disabled the vehicles system for automatically applying brakes in dangerous situations despite these fatal errors alphabet incs waymo recently announced plans for an early rider program in phoenix residents can sign up to be waymo test subjects and be driven automatically in the process many claim that the occasional autonomous vehicle fatality needs to be put in the context of the existing ecosystem in which many drivingrelated deaths happen without ai however because regulations and liability regimes govern humans and machines differently risks generated from machinehuman interactions do not cleanly fall into a discrete regulatory or accountability category strong incentives for regulatory and jurisdictional arbitrage exist in this and many other ai domains for example the fact that phoenix serves as the site of waymo and uber testing is not an accident early this year arizona perhaps swayed by a promise of technology jobs and capital made official what the state allowed in practice since fully autonomous vehicles without anyone behind the wheel are permitted on public roads this policy was put in place without any of the regulatory scaffolding that would be required to contend with the complex issues that are raised in terms of liability and accountability in the words of the phoenix new times arizona has agreed to step aside and see how this technology develops if something goes wrong well theres no plan for that yet this regulatory accountability gap is clearly visible in the uber death case apparently caused by a combination of corporate expedience disabling the automatic braking system and backup driver distraction while autonomous vehicles arguably present ais most straightforward nonmilitary dangers to human safety other ai domains also raise serious concerns for example ibms watson for oncology is already being tested in hospitals across the globe assisting in patient diagnostics and clinical care increasingly its effectiveness and the promises of ibms marketing are being questioned investigative reporters gained access to internal documents that paint a troubling picture of ibms system including its recommending unsafe and incorrect cancer treatments while this system was still in its trial phase it raised serious concerns about the incentives driving the rush to integrate such technology and the lack of clinical validation and peerreviewed research attesting to ibms marketing claims of effectiveness such events have not slowed ai deployment in healthcare recently the us food and drug administration fda issued a controversial decision to clear the new apple watch which features a builtin electrocardiogram ekg and the ability to notify a user of irregular heart rhythm as safe for consumers here concerns that the fda may be moving too quickly in an attempt to keep up with the pace of innovation have joined with concerns around data privacy and security similarly deepmind healths decision to move its streams application a tool designed to support decisionmaking by nurses and health practitioners under the umbrella of google caused some to worry that deepminds promise to not share the data of patients would be broken children and young adults are frequently subjects of such experiments earlier this year it was revealed that pearson a major aieducation vendor inserted socialpsychological interventions into one of its commercial learning software programs to test how students would respond they did this without the consent or knowledge of students parents or teachers the company then tracked whether students who received growthmindset messages through the learning software attempted and completed more problems than students who did not this psychological testing on unknowing populations especially young people in the education system raises significant ethical and privacy concerns it also highlights the growing influence of private companies in purportedly public domains and the lack of transparency and due process that accompany the current practices of ai deployment and integration here we see not only examples of the real harms that can come from biased and inaccurate ai systems but evidence of the ai industrys willingness to conduct early releases of experimental tools on human populations as amazon recently responded when criticized for monetizing peoples wedding and baby registries with deceptive advertising tactics were constantly experimenting this is a repeated pattern when market dominance and profits are valued over safety transparency and assurance without meaningful accountability frameworks as well as strong regulatory structures this kind of unchecked experimentation will only expand in size and scale and the potential hazards will grow emerging solutions in bias busting and formulas for fairness the limits of technological fixes over the past year we have seen growing consensus that ai systems perpetuate and amplify bias and that computational methods are not inherently neutral and objective this recognition comes in the wake of a string of examples including evidence of bias in algorithmic pretrial risk assessments and hiring algorithms and has been aided by the work of the fairness accountability and transparency in machine learning community the community has been at the center of an emerging body of academic research on airelated bias and fairness producing insights into the nature of these issues along with methods aimed at remediating bias these approaches are now being operationalized in industrial settings in the search for algorithmic fairness many definitions of fairness along with strategies to achieve it have been proposed over the past few years primarily by the technical community this work has informed the development of new algorithms and statistical techniques that aim to diagnose and mitigate bias the success of such techniques is generally measured against one or another computational definition of fairness based on a mathematical set of results however the problems these techniques ultimately aim to remedy have deep social and historical roots some of which are more cleanly captured by discrete mathematical representations than others below is a brief survey of some of the more prominent approaches to understanding and defining issues involving algorithmic bias and fairness allocative harms describe the effects of ai systems that unfairly withhold services resources or opportunities from some such harms have captured much of the attention of those dedicated to building technical interventions that ensure fair ai systems in part because it is theoretically possible to quantify such harms and their remediation however we have seen less attention paid to fixing systems that amplify and reproduce representational harms the harm caused by systems that reproduce and amplify harmful stereotypes often doing so in ways that mirror assumptions used to justify discrimination and inequality in a keynote of the conference on neural information processing neurips ai now cofounder kate crawford described the way in which historical patterns of discrimination and classification which often construct harmful representations of people based on perceived differences are reflected in the assumptions and data that inform ai systems often resulting in allocative harms this perspective requires one to move beyond locating biases in an algorithm or dataset and to consider the role of ai in harmful representations of human identity and the way in which such harmful representations are both shaped and shape our social and cultural understandings of ourselves and each other observational fairness strategies attempt to diagnose and mitigate bias by considering a dataset either data used for training an ai model or the input data processed by such a model and applying methods to the data aimed at detecting whether it encodes bias against individuals or groups based on characteristics such as race gender or socioeconomic standing these characteristics are typically referred to as protected or sensitive attributes the majority of observational fairness approaches can be categorized as being a form of either anticlassification classification parity or calibration as proposed by sam corbettdavies and sharad goel observational fairness strategies have increasingly emerged through efforts from the community to contend with the limitations of technical fairness work and to provide entry points for other disciplines anticlassification strategies declare a machine learning model to be fair if it does not depend on protected attributes in the data set for instance this strategy considers a pretrial risk assessment of two defendants who differ based on race or gender but are identical in terms of their other personal information to be fair if they are assigned the same risk this strategy often requires omitting all protected attributes and their proxies from the data set that is used to train a model proxies being any attributes that are correlated to protected attributes such as zip code being correlated with race classification parity declares a model fair when its predictive performance is equal across groupings that are defined by protected attributes for example classification parity would ensure that the percentage of people an algorithm turns down for a loan when they are actually creditworthy its false negative rate is the same for both black and white populations in practice this strategy often results in decreasing the accuracy for certain populations in order to match that of others calibration strategies look less at the data and more at the outcome once an ai system has produced a decision or prediction these approaches work to ensure that outcomes do not depend on protected attributes for example in the case of pretrial risk assessment applying a calibration strategy would aim to make sure that among a pool of defendants with a similar risk score the proportion who actually do reoffend on release is the same across different protected attributes such as race several scholars have identified limitations with these approaches to observational fairness with respect to anticlassification some argue that there are important cases where protected attributessuch as race or gender should be included in data used to train and inform an ai system in order to ensure equitable decisions for example corbettdavies and goel discuss the importance of including gender in pretrial risk assessment as women reoffend less often than men in many jurisdictions genderneutral risk assessments tend to overstate the recidivism risk of women which can lead to unnecessarily harsh judicial decisions as a result some jurisdictions use genderspecific risk assessment tools these cases counter a widespread view that deleting sufficient information from data sets will eventually debias an ai system since correlations between variables in a dataset almost always exist removing such variables can result in very little information and thus poor predictive performance without the ability to measure potential harms post hoc secondly some have argued that different mathematical fairness criteria are mutually exclusive hence it is generally not possible except in highly constrained cases to simultaneously satisfy both calibration and any form of classification parity these impossibility results show how each fairness strategy makes implicit assumptions about what is and is not fair they also highlight the inherent mathematical tradeoffs facing those aiming to mitigate various forms of bias based on one or another fairness definition ultimately these findings serve to complicate the broader policy debate focused on solving bias issues with mathematical fairness tools what they make clear is that solving complex policy issues related to bias and discrimination by indiscriminately applying one or more fairness metrics is unlikely to be successful this does not mean that such metrics are not useful observational criteria may help understanding around whether datasets and ai systems meet various notions of fairness and bias and subsequently help inform a richer discussion about the goals one hopes to achieve when deploying ai systems in complex social contexts the proliferation of observational fairness methods also raises concerns over the potential to provide a false sense of assurance while researchers often have a nuanced sense of the limitations of their tools others who might implement them may ignore such limits when looking for quick fixes the idea that once treated with such methods ai systems are free of bias and safe to use in sensitive domains can provide a dangerous sense of false securityone that relies heavily on mathematical definitions of fairness without looking at the deeper social and historical context as legal scholar frank pasquale observes algorithms alone cant meaningfully hold other algorithms accountable while increased attention to the problems of fairness and bias in ai is a positive development some have expressed concern over a mathematization of ethics as shira mitchell has argued as statistical thinkers in the political sphere we should be aware of the hazards of supplanting politics by an expert discourse in general every statistical intervention to a conversation tends to raise the technical bar of entry until it is reduced to a conversation between technical expertsare we speaking statistics to power or are we merely providing that power with new tools for the marginalization of unquantified political concerns such concerns are not new upcoming work by hutchinson and mitchell surveys over fifty years of attempts to construct quantitative fairness definitions across multiple disciplines their work recalls a period between and when researchers focused on defining fairness for educational assessments in ways that echo the current ai fairness debate their efforts stalled after they were unable to agree on broad technical solutions to the issues involved in fairness these precedents emphasize what the fairness accountability and transparency in machine learning community has been discovering without a tight connection to real world impact the added value of new fairness metrics and algorithms in the machine learning community could be minimal in order to arrive at more meaningful research on fairness and algorithmic bias we must continue to pair the expertise and perspectives of communities outside of technical disciplines to those within broader approaches dobbe et al have drawn on the definition of bias proposed in the early valuesensitive design vsd literature to propose a broader view of fairness vsd as theorized in the nineties by batya friedman and helen nissenbaum asserts that bias in computer systems preexists the system itself such bias is reflected in the data that informs the systems and embedded in the assumptions made during the construction of a computer system this bias manifests during the operation of the systems due to feedback loops and dissonance between the system and our dynamic social and cultural contexts the vsd approach is one way to bring a broader lens to these issues emphasizing the interests and perspectives of direct and indirect stakeholders throughout the design process another approach is a social systems analysis first described by kate crawford and ryan calo in nature this is a method that combines quantitative and qualitative research methods by forensically analyzing a technical system while also studying the technology once it is deployed in social settings it proposes that we engage with social impacts at every stageconception design deployment and regulation of a technology across the life cycle we have also seen increased focus on examining the provenance and construction of the data used to train and inform ai systems this data shapes ai systems view of the world and an understanding of how it is created and what it is meant to represent is essential to understanding the limits of the systems that it informs as an initial remedy to this problem a group of researchers led by timnit gebru proposed datasheets for datasets a standardized form of documentation meant to accompany datasets used to train and inform ai systems a followup paper looks at standardizing provenance for ai models these approaches allow ai practitioners and those overseeing and assessing the applicability of ai within a given context to better understand whether the data that shapes a given model is appropriate representative or potentially possessing legal or ethical issues advances in biasbusting and fairness formulas are strong signs that the field of ai has accepted that these concerns are real however the limits of narrow mathematical models will continue to undermine these approaches until broader perspectives are included approaches to fairness and bias must take into account both allocative and representational harms and those that debate the definitions of fairness and bias must recognize and give voice to the individuals and communities most affected any formulation of fairness that excludes impacted populations and the institutional context in which a system is deployed is too limited industry applications toolkits and system tweaks this year we have also seen several technology companies operationalize fairness definitions metrics and tools in the last year four of the biggest ai companies released bias mitigation tools ibm released the ai fairness opensource tool kit which includes nine different algorithms and many other fairness metrics developed by researchers in the fairness accountability and transparency in machine learning community the toolkit is intended to be integrated into the software development pipeline from early stages of data preprocessing to the training process itself through the use of specific mathematical models that deploy bias mitigation strategies googles people ai research group pair released the opensource whatif tool a dashboard allowing researchers to visualize the effects of different bias mitigation strategies and metrics as well as a tool called facets that supports decisionmaking around which fairness metric to use microsoft released fairlearnpy a python package meant to help implement a binary classifier subject to a developers intended fairness constraint facebook announced the creation and testing of a tool called fairness flow an internal tool for facebook engineers that incorporates many of the same algorithms to help identify bias in machine learning models even accenture a consulting firm has developed internal software tools to help clients understand and essentially eliminate the bias in algorithms industry standards bodies have also taken on fairness efforts in response to industry and public sector requests for accountability assurances the institute of electrical and electronics engineers ieee recently announced an ethics certification program for autonomous and intelligent systems in the hopes of creating marks that can attest to the broader public that an ai system is transparent accountable and fair while this effort is new and while ieee has not published the certifications underlying methods it is hard to see given the complexity of these issues how settling on one certification standard across all contexts and all ai systems would be possibleor ultimately reliablein ensuring that systems are used in safe and ethical ways similar concerns have arisen in other contexts such as privacy certification programs in both the rapid industrial adoption of academic fairness methods and the rush to certification we see an eagerness to solve and eliminate problems of bias and fairness using familiar approaches and skills that avoid the need for significant structural change and which fail to interrogate the complex social and historical factors at play combining academically credible technical fairness fixes and certification check boxes runs the risk of instrumenting fairness in ways that lets industry say it has fixed these problems and may divert attention from examining ongoing harms it also relieves companies of the responsibility to explore more complex and costly forms of review and remediation rather than relying on quick fixes tools and certifications issues of bias and fairness require deeper consideration and more robust accountability frameworks including strong disclaimers about how automated fairness cannot be relied on to truly eliminate bias from ai systems why ethics is not enough a toplevel recommendation in the ai now report advised that ethical codes meant to steer the ai field should be accompanied by strong oversight and accountability mechanisms while we have seen a rush to adopt such codes in many instances offered as a means to address the growing controversy surrounding the design and implementation of ai systems we have not seen strong oversight and accountability to backstop these ethical commitments after it was revealed that google was working with the pentagon on project mavendeveloping ai systems for drone surveillancethe debate about the role of ai in weapons systems grew in intensity project maven generated significant protest among googles employees who successfully petitioned the companys leadership to end their involvement with the program when the current contract expired by way of response googles ceo sundar pichai released a public set of seven guiding principles designed to ensure that the companys work on ai will be socially responsible these ethical principles include the commitment to be socially beneficial and to avoid creating or reinforcing unfair bias they also include a section titled ai applications we will not pursue which includes weapons and other technologies whose principal purpose or implementation is to cause or directly facilitate injury to peoplea direct response to the companys decision not to renew its contract with the department of defense but it is not clear to the public who would oversee the implementation of the principles and no ethics board has been named google was not alone other companies including microsoft facebook and police body camera maker axon also assembled ethics boards advisors and teams in addition technical membership organizations moved to update several of their ethical codes the ieee reworked its code of ethics to reflect the challenges of ai and autonomous systems and researchers in the association for computing machinery acm called for a restructuring of peer review processes requiring the authors of technical papers to consider the potential adverse uses of their work which is not a common practice universities including harvard nyu stanford and mit offered new courses on the ethics and ethical ai development practices aimed at identifying issues and considering the ramifications of technological innovation before it is implemented at scale the university of montreal launched a wideranging process to formulate a declaration for the responsible development of ai that includes both expert summits and open public deliberations for input from citizens such developments are encouraging and it is noteworthy that those at the heart of ai development have declared they are taking ethics seriously ethical initiatives help develop a shared language with which to discuss and debate social and political concerns they provide developers company employees and other stakeholders a set of highlevel value statements or objectives against which actions can be later judged they are also educational often doing the work of raising awareness of particular risks of ai both within a given institution and externally amongst the broader concerned public however developing socially just and equitable ai systems will require more than ethical language however wellintentioned it may be we see two classes of problems with this current approach to ethics the first has to do with enforcement and accountability ethical approaches in industry implicitly ask that the public simply take corporations at their word when they say they will guide their conduct in ethical ways while the public may be able to compare a post hoc decision made by a company to its guiding principles this does not allow insight into decision making or the power to reverse or guide such a decision in her analysis of googles ai principles lucy suchman a pioneering scholar of human computer interaction argues that without the requisite bodies for deliberation appeal and redress vague ethical principles like dont be evil or do the right thing are vacuous this trust us form of corporate selfgovernance also has the potential to displace or forestall more comprehensive and binding forms of governmental regulation ben wagner of the vienna university of economics and business argues unable or unwilling to properly provide regulatory solutions ethics is seen as the easy or soft option which can help structure and give meaning to existing selfregulatory initiatives in other words ethical codes may deflect criticism by acknowledging that problems exist without ceding any power to regulate or transform the way technology is developed and applied the fact that a former facebook operations manager claims we cant trust facebook to regulate itself should be taken into account when evaluating ethical codes in industry a second problem relates to the deeper assumptions and worldviews of the designers of ethical codes in the technology industry in response to the proliferation of corporate ethics initiatives greene et al undertook a systematic critical review of highprofile vision statements for ethical ai one of their findings was that these statements tend to adopt a technologically deterministic worldview one where ethical agency and decision making was delegated to experts a narrow circle of who can or should adjudicate ethical concerns around aiml on behalf of the rest of us these statements often assert that ai promises both great benefits and risks to a universal humanity without acknowledgement of more specific risks to marginalized populations rather than asking fundamental ethical and political questions about whether ai systems should be built these documents implicitly frame technological progress as inevitable calling for better building empirical study of the use of these codes is only beginning but preliminary results are not promising one recent study found that explicitly instructing engineers to consider the acm code of ethics in their decision making had no observed effect when compared with a control group however these researchers did find that media or historical accounts of ethical controversies in engineering like volkswagens dieselgate may prompt more reflective practice perhaps the most revealing evidence of the limitations of these emerging ethical codes is how corporations act after they formulate them among the list of applications google promises not to pursue as a part of its ai principles are technologies whose purpose contravenes widely accepted principles of international law and human rights that was tested earlier this year after investigative journalists revealed that google was quietly developing a censored version of its search engine which relies extensively on ai capabilities for the chinese market codenamed dragonfly organizations condemned the project as a violation of human rights law and as such a violation of googles ai principles google employees also organized against the effort as of writing the project has not been cancelled nor has its continued development been explained in light of the clear commitment in the companys ai principles although googles ceo has defended it as exploratory there is an obvious need for accountability and oversight in the industry and so far the move toward ethics is not meeting this need this is likely in part due to the marketdriven incentives working against industrydriven implementations a drastic if momentary drop in facebook and twitters share price occurred after they announced efforts to combat misinformation and increase spending on security and privacy efforts this is no excuse not to pursue a more ethically driven agenda but it does suggest that we should be wary of relying on companies to implement ethical practices voluntarily since many of the incentives governing these large publicly traded technology corporations penalize ethical action for these mechanisms to serve as meaningful forms of accountability requires that external oversight and transparency be put into place to ensure that there exists an external system of checks and balances in addition to the cultivation of ethical norms and values within the engineering profession and technology companies what is needed next when we released our ai now report fairness formulas debiasing toolkits and ethical guidelines for ai were rare the fact that they are commonplace today shows how far the field has come yet much more needs to be done below we outline seven strategies for future progress on these issues from fairness to justice any debate about bias and fairness should approach issues of power and hierarchy looking at who is in a position to produce and profit from these systems whose values are embedded in these systems who sets their objective functions and which contexts they are intended to work within echoing the association for computing machinery acm researchers call for an acknowledgement of negative implications as a requirement for peer review much more attention must be paid to the ways that ai can be used as a tool for exploitation and control we must also be cautious not to reframe political questions as technical concerns when framed as technical fixes debiasing solutions rarely allow for questions about the appropriateness or efficacy of an ai system altogether or for an interrogation of the institutional context into which the fixed ai system will ultimately be applied for example a debiased predictive algorithm that accurately forecasts where crime will occur but that is being used by law enforcement to harass and oppress communities of color is still an essentially unfair system to this end our definitions of fairness must expand to encompass the structural historical and political contexts in which an algorithmic systems is deployed furthermore fairness is a term that can be easily coopted important questions such as fair to whom and in what context should always be asked for example making a facial recognition system perform equally on people with light and dark skin may be a type of technical progress in terms of parity but if that technology is disproportionately used on people of color and lowincome communities is it really fair this is why definitions of fairness face a hard limit if they remain purely contained within the technical domain in short parity is not justice infrastructural thinking in order to better understand and track the complexities of ai systems we need to look beyond the technology and the hype to account for the broader context of how ai is shaping and shaped by social and material forces as edwards et al argue when dealing with infrastructures we need to look to the whole array of organizational forms practices and institutions which accompany make possible and inflect the development of new technology doing so requires both experimental methodological approaches and theory building expanding beyond narrow analyses of individual systems in isolation to consider them on a local and global scale it also requires considering ways in which technologies are entangled in social relations material dependencies and political purposes in anatomy of an ai system a essay and largescale map ai now cofounder kate crawford and professor vladan joler took a single amazon echo and analyzed all the forms of environmental and labor resources required to develop produce maintain and finally dispose of this sleek and seemingly simple object when you ask alexa to play your favorite song you have drawn on a massive interlinked chain of extractive processes it involves lithium mining in bolivia clickworkers creating largescale training datasets in southeast asia container ships and international logistics and vast data extraction and analysis by alexa voice service avs across distributed data centers the process ends in the final resting place of all ai consumer gadgets in ewaste rubbish heaps in ghana pakistan and china the anatomy of an ai system project points to approaches we can employ in contending with the global implications of ai and the multilayered nature of value extraction and exploitation from the developing world to the developed world this helps to illuminate the darker corners that are rarely considered in analysis of ai systems in particular an infrastructural analysis of ai shows that there are black boxes within black boxes not just at the algorithmic level but also at the levels of trade secrecy laws labor practices and untraceable global supply chains for rare earth minerals used to build consumer ai devices these obscure not only the material impacts of ai systems but the intensive human work of maintaining and repairing them through practices like content moderation and data training as nick seaver puts it if you cannot see a human in the loop you just need to look for a bigger loop only by tracing across these sociotechnical layers can we understand what we are calling the full stack supply chain of aithe human and nonhuman components that make up the global scale of ai systems there are many sociotechnical data infrastructures needed to make ai function these include training data test data apis data centers fiber networks undersea cables energy use labor involved in content moderation and training set creation and a constant reliance on clickwork to develop and maintain ai systems we cannot see the global environmental and labor implications of these tools of everyday convenience nor can we meaningfully advocate for fairness accountability and transparency in ai systems without an understanding of this full stack supply chain accounting for hidden labor in ai systems another emerging research area where we expect to see greater impact focuses on the underpaid and unrecognized workers who help build maintain and test ai systems this hidden human labor takes many formsfrom supply chain work to digital crowdsourced clickwork to traditional service industry jobs hidden labor exists at all stages of the ai pipeline from producing and transporting the raw minerals required to create the core infrastructure of ai systems to providing the invisible human work that often backstops claims of ai magic once these systems are deployed in products and services communications scholar lilly irani refers to such hidden labor as humanfueled automation her research draws attention to the experiences of clickworkers or microworkers who perform the repetitive digital tasks that underlie ai systems like labeling training data and reviewing flagged content as workers hidden in the technology while this labor is essential to making ai systems work it is usually very poorly compensated a study from the united nations international labor organization ilo surveyed microworkers from countries who routinely offered their labor on popular microtask platforms like mechanical turk crowdflower microworker and clickworker the report found that a substantial number of people earned below their local minimum wage despite of respondents having advanced degrees specializing in science and technology similarly those who do content moderation work screening problematic content posted on social media platforms and news feeds are also paid poorly in spite of their essential and emotionally difficult labor this has not been lost on some in the technical ai research community who have begun to call attention to the crucial and marginalized role of this labor and to consider their own responsibility to intervene silberman and others discuss how researchers conducting ai studies are increasingly dependent upon cheap crowdsourced labor they note that between the years and the term crowdsourcing went from appearing in less than scientific articles to over with online microworkers unregulated by current labor laws researchers are being asked to reconsider what counts as ethical conduct in the ai research community silberman et al argue for treating crowdworkers as coworkers paying them minimum wage determined by the clients location and the need for additional institutional review board irb oversight the practice of examining hidden human labor draws on a lineage of feminist research the concept of invisible work for instance originated with studies of unpaid womens care work and investigations into organizational settings that relied upon emotional labor particularly traditionally feminized fields like nursing and flight attendants researchers found that common activities taken on by female workers such as soothing anxious patients or managing unruly customers were not formally recognized or compensated as work in spite of their being essential the feminist legacy of invisible work is useful for contextualizing these new forms of labor and in understanding the characterization of this work which while essential is often written out of the ai narrative rarely counted or compensated in her article the automation charade astra taylor proposes the term fauxtomation to call attention to the gap between the marketing rhetoric of ai as a seamless product or service and the messy lived reality of automation which frequently relies on such unsung human labor automation taylor cautions has an ideological function as well as a technological dimension in making this case she critiques popular narratives around the future of labor which posit a nearhorizon where workers will be replaced by machines she sees such claims as functioning to disempower workers what leverage do workers have to demand better wages and benefits in the face of impending automation we saw this narrative deployed in by former mcdonalds ceo ed rensi who cited the growing fight for movement as the impetus for the companys introduction of automated kiosks to replace cashiers workers who fought for better pay would ultimately be worse off he reasoned as their demand for living wages would force the company to automate and eliminate them examining his claim two years on we see that this is not entirely true automation or no workers are still needed after mcdonalds added kiosks to its chicago flagship store the location reopened with more employees than before the kiosks were introduced the integration of automation and ai in the workplace is aimed not only at automating worker tasks but at managing monitoring and assessing workers themselves alex rosenblatts ethnography of uber drivers details the precarity and uncertainty produced by depending on the whims of a centralized aienabled platform for ones livelihood the algorithmic logic that governs ridesharing applications can arbitrarily bar drivers from work result in unreliable wages and unexpected costs and nudge people into working longer hours resulting in unsafe driving conditions such platforms isolate workers from each other making concerted activity and labor organizing difficult they also function to create significant information asymmetries between datarich companies aiming to extract value from workers and the workers themselves even so has seen increasing dissent from such workers some prominent examples of workerdriven protest include ondemand delivery riders striking alongside uk fast food industry employees and rideshare drivers calling for job protections silicon valley contractors working in security food and janitorial services within major technology companies have also organized seeking a living wage and other protections they are among thousands of workers who labor alongside their fulltime technology worker peers but are classified as independent contractors under this designation they are often paid low wages and provided few benefits and protections they are also rarely counted in official employee numbers even though they make up a large portion of most technology industry workforces and perform essential work for example as of this year contract workers outnumber googles direct employees for the first time in the companys history this increasing wave of dissent makes visible the social tensions at the heart of the practice of hiding and marginalizing important forms of labor the physical emotional and financial costs of treating workers like bits of code and devaluing their work and wellbeing has been highlighted in recent news articles describing the conditions of amazon warehouse workers and contracted prime delivery drivers amazon warehouse workers recently went on strike in europe protesting harsh conditions according to one striking worker you start at the company healthy and leave it as a broken human with many workers requiring surgeries related to workplace conditions recognizing all of the labor required to make ai work can help us better understand the implications of its development and use research in these areas also helps us reexamine the focus on technical talent in narratives describing ais creation and recognize that technical skills account for only a portion of a much larger effort this enables us to question numerous labor policies such as the focus on pushing workers to acquire coding or data science skills as a way to ensure they are counted and compensated they also help us identify who is likely to benefit and who along the ai production and deployment pipeline is likely to be harmed deeper interdisciplinarity ai researchers and developers are engaged in building technologies that have significant implications for diverse populations in broad fields like law sociology and medicine yet much of this development happens far removed from the experience and expertise of these groups this has led to a call to expand the disciplinary makeup of those engaged in ai design development and critique beyond purely technical expertise since then we have seen some movement in this direction recently mit announced plans to establish a new college of computing that aims to advance pioneering work on ais ethical use and societal impact by fostering integrated crossdisciplinary training educating the bilinguals of the future as mit president l rafael reif described it such initiatives are critical as ai becomes more deeply embedded in areas like healthcare criminal justice hiring housing and educational systems experts from these domains are essential if we are to ensure ai works as envisioned in integrating these disciplinary perspectives it is important that they are not merely languages to be acquired by computer scientists and engineers seeking to expand their work into new areasespecially when other disciplines have been leading that work instead social science and the humanities should be centered as contributors to the ai fields foundational knowledge and future direction enabling us to leverage new modes of analysis and methodological intervention race gender and power in ai this year a groundswell of political action emerged around issues of discrimination harassment and inequity in the technology industry especially in the ai field this rising concern weaves together a number of related issues from the biases in ai systems to failed diversity and inclusion efforts within industry and academia to the grassroots efforts to confront sexual harassment and the abuse of power in workplaces and classrooms resonating with the broader metoo movement we saw issues relating to diversity and inclusion in artificial intelligence rise on the public agenda following the conference on neural information processing systems members of the artificial intelligence and machine learning communities began voicing concerns about long standing problems of harassment and discrimination in conference settings leading to protestnips a movement aimed at highlighting examples of toxicity in the community and the need to address them among other things this provoked a change to the conference acronym a longstanding subject of sensitivity for its gendered and historical connotations the conference which was previously referred to as nips now goes by neurips we also saw renewed focus on initiatives devoted to creating platforms for inclusion in the field such as black in ai women in machine learning latinx in ai and queer in ai alongside the appointment of diversity and inclusion chairs and a series of other changes to the design of neurips intended to foster equity and inclusion among participants across the industry we saw a growing technology worker movement that intersected with these issues the google walkout in particular took on a workerdriven agenda that acknowledged that race class and sexuality are intertwined with forms of genderbased discrimination the walkout explicitly aimed to center the needs of the companys temporary contract workers and vendors who lack the job security and benefits of more privileged technology workers these efforts have led to some significant structural changesnotably the end to forced arbitration for sexual harassment claims across a number of the largest companies in the ai industry in other arenas corporate boards have ignored or otherwise refused to address shareholder proposals targeting discriminatory workspaces this year google dismissed a proposal that would tie executive compensation to progress made on diversity and inclusion while in apple refused a mandate that would require it to diversify its board and senior management across these efforts advocates of diversity in ai are finding intersections between the move to address gender and racebased harassment and abuse within the technology community and other forms of inequity and abuses of power but this is still an uphill battle while there is increased attention to problems of bias in ai systems we have yet to see much research within the fairness and bias debate focused on the state of equity and diversity in the ai field itself indeed reliable figures on representation in ai are difficult to come by although some limited data does exist a recent estimate produced by wired and element ai found that only of researchers who contributed to the three leading machine learning conferences in were women this gender gap is replicated at large technology firms like facebook and google whose websites show that only and of their ai research staff are women and there is no reliable data on the state of racial diversity in the field or retention rates for people of color collectively the limited evidence suggests that ai as a field is even less diverse than computer science as a whole which is itself at a historic low point women make up only of computer science majors in the united states a decline from a high point of in these trends are even more dramatic when compared to other stem fields in which gender diversity has shown a marked improvement yet these are not new problems the wired element ai survey is not significantly different from a study of the ai field that was published by ieee expert in which found that only of published authors in the journal over the prior four years were women and in the s female grad students at mits computer science and artificial intelligence labs thoroughly documented their experiences with toxic working environments in the report barriers to equality in academia women in computer science at mit it is time to address the connection between discrimination and harassment in the ai community and bias in the technical products that are produced by the community scholars in science and technology studies have long observed that the values and beliefs of those who create technologies shape the technologies they create expanding the fields frame of reference to recognize this connection will ensure it is better equipped to address the problems raised by its rapid proliferation into sensitive social domains as one ai researcher put it bias is not just in our datasets its in our conferences and community a recent example illustrates these connections and how discriminatory practices within the culture that produces an ai system can be mirrored and amplified in the system itself amazon recently developed an experimental ai system to help it rank job candidates it trained the system on data reflecting the companys historical hiring preferences hoping to more efficiently identify qualified candidates but the system didnt work as expected based on the companys historical hiring it showed a distinct bias against women candidates downgrading resumes from candidates who attended two allwomens colleges and even penalizing resumes that contained the word woman after uncovering this bias the company attempted to fix the system adjusting the algorithm to treat these terms more fairly this did not work and the project was eventually scrapped genderbased discrimination was embedded too deeply within the system a system built to reflect amazons past hiring practices to be uprooted using the debiasing approach commonly adopted within the ai field as scholars like safiya noble and mar hicks have observed there is a clear throughline connecting longstanding patterns of discrimination and harassment in ai to the ways artificial intelligence technologies can amplify and contribute to marginalization and social inequity patterns of cultural discrimination are often embedded in ai systems in complex and meaningful ways and we need to better understand how these effects are felt by different communities this is a space that has too long been overlooked and where research is sorely needed ai now will be publishing a dedicated report on these issues and we have a multiyear research project dedicated to examining these challenges strategic litigation and policy interventions this year saw an increase in court challenges to the use of automated systems particularly when government agencies use them in decisions that affect individual rights in a recent ai now report called litigating algorithms we documented five recent case studies of litigation involving the use of automated systems in medicaid and disability benefits cases public teacher employment evaluations juvenile criminal risk assessment and criminal dna analysis the findings brought to light several emerging trends first these cases provided concrete evidence that governments are routinely adopting automated decision systems ads as measures to produce cost savings or to streamline work yet they are failing to assess how these systems might disproportionately harm the populations they are meant to serve particularly those who are the most vulnerable and who have little recourse or even knowledge that these systems are deeply affecting their lives in many cases there was not a single government employee who could explain the automated decision correct errors or audit the results of its determination through a series of vendor and contractor agreements almost all avenues for understanding or contesting the impact of these systems were shielded by legal protections such as trade secret law second few government agencies had invested real efforts to ensure that fairness and due process protections remained in place when switching from humandriven decisions to algorithmicallydriven ones the typical audit appeals and accountability mechanisms were totally absent from automated system design fortunately successful strategic litigation by lawyers from the american civil liberties union aclu of idaho legal aid of arkansas the houston federation of teachers the legal aid society of new york and various public defenders were able to secure victories for their clients and challenge these unlawful uses based in part on constitutional and administrative due process litigation claims the playbook for how to litigate algorithms is still being written but our report uncovered several useful strategies to support longterm solutions and protections first arguments based on procedural due process presented serious challenges to the trade secrecy claims of private vendors with the vast majority of judges ruling that the right to assert constitutional or civil rights protections outweighs any risk of intellectual property misappropriation second a failure to notify affected individuals and communities matters agencies who neglected to engage community groups concerning the use of these systems were often judged to have failed to appropriately provide the opportunity for public notice and comment meaning that their implementation of ai systems was potentially unconstitutional third interdisciplinary collaboration is important when trying to determine where these systems fail especially when submitting evidence to judges in cases in which lawyers worked closely with technical and social science experts judges were able to learn about the scientific flaws in these systems as well as the social ramifications and harms looking forward we anticipate future strategic litigation cases will produce many more lessons these interventions generate greater understanding and remedial accountability for these systems even in situations where government agencies have attempted to disclaim ownership understanding or control combined with tools such as ai nows algorithmic impact assessment framework alongside robust regulatory oversight regimes we can begin to identify measure and when necessary intervene in efforts to use ai and automated systems in ways that produce harm however in order to continue to build on recent progress lawyers and community activists who represent individuals in such suits need greater funding and support as well as networks of domain experts that they can draw on to help advise strategy and audit systems research and organizing an emergent coalition the rapid deployment of ai and related systems in everyday life is not a concern for the futureit is already here with no signs of slowing down recognizing this a set of strategies have emerged drawing on longstanding traditions of activism and organizing to demand structural changes for greater accountability social activism by technologists is nothing new in the early s computer professionals for social responsibility formed to oppose the use of computers in warfare more recently the never again technology pledge rallied thousands of workers in various technology sectors to sign a promise not to build databases or conduct data collection that could be used to target religious minorities or facilitate mass deportations while s organizing and activism draws from a long tradition its scale is new to the technology sector technology workers are joining forces with civil society organizations and researchers in opposition to their employers technical and business decisions google employees kicked off publicly visible organizing in opposing project maven a pentagon effort to apply googles machine vision ai capabilities to department of defense drone surveillance researchers and human rights organizations joined the cause and in june google announced it would abandon the project at amazon salesforce and microsoft employees petitioned their leadership to end contracts with immigrations and customs enforcement ice supported by immigration and advocacy organizations amazon employees also joined the aclu in petitioning the company to stop selling facial recognition to law enforcement responding to the aclus work exposing existing contracts following maven google employees again rose up against project dragonfly a version of the google search engine enabling governmentdirected censorship and surveillance planned for the chinese market in response to media reports that disclosed the secretive effort employees requested ethical oversight and accountability and over of them joined amnesty international in a call to cancel the project signing their name publicly to an open letter which coincided with amnesty international protests the biggest moment occurred in early november when google workers walked out around the globe in an action called walkout for real change the walkout characterized google as a company at which abuse of power systemic racism and unaccountable decisionmaking are the norm organizers called on leadership to meet five demands including ending pay and opportunity inequity eliminating forced arbitration in cases of sexual harassment and discrimination and adding an employee representative to the board of directors a week after the walkout google met a small portion of these demands agreeing to end forced arbitration in cases of sexual harassment but notably ignoring discrimination this move was quickly replicated throughout the industry with facebook square ebay and airbnb following suit by joining forces with researchers and civil society groups this new wave of labor organizing mirrors calls for greater diversity and openness within the ai research domain these movements are incorporating diverse perspectives across class sector and discipline working to ensure they are capable of understanding the true costs of company practices including the impact of the systems they build the google workers who participated in the walkout expanded their coalition across class and sector emphasizing contract workers in their demands and situating themselves within a growing movement not just in tech but across the country including teachers fastfood workers and others who are using their strength in numbers to make real change the recent surge in activism has largely been driven by whistleblowers within technology companies who have disclosed information about secretive projects to journalists these disclosures have helped educate the public which is traditionally excluded from such access and helped external researchers and advocates provide more informed analysis by establishing shared ground truth whistleblowing has helped build the broad coalitions that characterize these movements the critical role of ethical whistleblowing over the last year has also highlighted both its social importance and the lack of protections for those who make such disclosures the broad coalition of technology worker organizers researchers and civil society is playing an increasing role in the push for accountability in the technology sector many engineering employees have considerable bargaining power and are uniquely positioned to demand change from their employers applying this power to push for greater accountability presents a hopeful model for labor organizing in the public interest especially given the current lack of government regulation external oversight and other meaningful levers capable of reviewing and steering technology company decision making conclusion this year saw ai systems rapidly introduced into more social domains leaving increasing numbers of people at risk while ai techniques still offer considerable promise rapid deployment of systems without appropriate assessment accountability and oversight can create serious hazards we urgently need to regulate ai systems sectorbysector with particular attention paid to facial and affect recognition and to inform those policies with rigorous research but regulation can only be effective if the legal and technological barriers that prevent auditing understanding and intervening in these systems are removed back in we recommended in the first ai now report that the computer fraud and abuse act cfaa and the digital millennium copyright act dmca should not be used to restrict research into ai accountability and auditing this year we go further ai companies should waive trade secrecy and other legal claims that would prevent algorithmic accountability in the public sector governments and public institutions must be able to understand and explain how and why decisions are made particularly when peoples access to healthcare housing and employment is on the line the question is no longer whether there are harms and biases in ai systems that debate has been settled the evidence has mounted beyond doubt in the last year the next task now is addressing these harms this is particularly urgent given the scale at which these systems are deployed the way they function to centralize power and insight in the hands of the few and the increasingly uneven distribution of costs and benefits that accompanies this centralization we need deeper analyses of the full stack supply chain behind ai systems to track their development and deployment across the product life cycle and to take into account their true environmental and labor costs furthermore it is long overdue for technology companies to directly address the cultures of exclusion and discrimination in the workplace the lack of diversity and ongoing tactics of harassment exclusion and unequal pay are not only deeply harmful to employees in these companies but also impacts the ai products they release producing tools that perpetuate bias and discrimination the current structure within which ai development and deployment occurs works against meaningfully addressing these pressing issues those in a position to profit are incentivized to accelerate the development and application of systems without taking the time to build diverse teams create safety guardrails or test for disparate impacts those most exposed to harm from these systems commonly lack the financial means and access to accountability mechanisms that would allow for redress or legal appeals this is why we are arguing for greater funding for public litigation labor organizing and community participation as more ai and algorithmic systems shift the balance of power across many institutions and workplaces it is imperative that the balance of power shifts back in the publics favor this will require significant structural change that goes well beyond a focus on technical systems including a willingness to alter the standard operational assumptions that govern the modern ai industry players the current focus on discrete technical fixes to systems should expand to draw on sociallyengaged disciplines histories and strategies capable of providing a deeper understanding of the various social contexts that shape the development and use of ai systems as more universities turn their focus to the study of ais social implications computer science and engineering can no longer be the unquestioned center but should collaborate more equally with social and humanistic disciplines as well as with civil society organizations and affected communities fortunately we are beginning to see new coalitions form between researchers activists lawyers concerned technology workers and civil society organizations to support the oversight accountability and ongoing monitoring of ai systems for these important connections to grow more protections are needed including a commitment from technology companies to provide protections for conscientious objectors who do not want to work on military or policing contracts along with protections for employees involved in labor organizing and ethical whistleblowers the last year revealed many of the hardest challenges for accountability and justice as ai systems moved deeper into the social world yet there have been extraordinary moments of potential as well as significant public debates and hopeful forms of protest that may ultimately illuminate the pathways for consequential and positive change